{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-15T03:30:45.764681Z",
     "iopub.status.busy": "2025-04-15T03:30:45.764040Z",
     "iopub.status.idle": "2025-04-15T03:33:26.323766Z",
     "shell.execute_reply": "2025-04-15T03:33:26.322955Z",
     "shell.execute_reply.started": "2025-04-15T03:30:45.764645Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 199MB/s]\n",
      "100%|██████████| 90/90 [00:18<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.6483, Train Acc: 64.29%, Val Loss: 0.6108, Val Acc: 66.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:09<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 0.5009, Train Acc: 76.40%, Val Loss: 0.6260, Val Acc: 68.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:09<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 0.3770, Train Acc: 83.26%, Val Loss: 0.7362, Val Acc: 64.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:09<00:00,  9.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 0.2967, Train Acc: 87.61%, Val Loss: 1.2123, Val Acc: 60.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:09<00:00,  9.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 0.2120, Train Acc: 92.72%, Val Loss: 0.8776, Val Acc: 67.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:09<00:00,  9.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 0.1559, Train Acc: 94.40%, Val Loss: 1.2689, Val Acc: 64.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:09<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 0.1925, Train Acc: 92.93%, Val Loss: 1.0836, Val Acc: 61.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:09<00:00,  9.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 0.1334, Train Acc: 95.24%, Val Loss: 1.1398, Val Acc: 65.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:09<00:00,  9.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 0.0972, Train Acc: 97.90%, Val Loss: 1.3539, Val Acc: 60.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90/90 [00:09<00:00,  9.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 0.1372, Train Acc: 95.24%, Val Loss: 0.9830, Val Acc: 66.01%\n",
      "Test Accuracy: 65.80%\n",
      "Confusion Matrix:\n",
      "[[123  38]\n",
      " [ 67  79]]\n",
      "Generating SHAP explanations...\n",
      "Generating descriptions based on SHAP values...\n",
      "Visualizing results for image 1/5...\n",
      "Visualizing results for image 2/5...\n",
      "Visualizing results for image 3/5...\n",
      "Visualizing results for image 4/5...\n",
      "Visualizing results for image 5/5...\n",
      "Done! Results saved to: outputs/\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "import shap\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    DATA_ROOT = \"/kaggle/input/real-vs-fake-faces/\"\n",
    "    REAL_IMAGES = DATA_ROOT + \"real/\"\n",
    "    FAKE_IMAGES = DATA_ROOT + \"fake/\"\n",
    "    OUTPUT_DIR = \"outputs/\"\n",
    "    \n",
    "    BATCH_SIZE = 16\n",
    "    IMAGE_SIZE = 224\n",
    "    NUM_EPOCHS = 10\n",
    "    LEARNING_RATE = 3e-4\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    NUM_BACKGROUND = 50\n",
    "    SHAP_SAMPLES = 25\n",
    "\n",
    "\n",
    "\n",
    "# 1. Data Loading and Preparation\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                image = img.convert('RGB')\n",
    "                if self.transform:\n",
    "                    image = self.transform(image)\n",
    "                label = self.labels[idx]\n",
    "                return image, label, img_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            dummy_img = Image.new('RGB', (Config.IMAGE_SIZE, Config.IMAGE_SIZE), color='black')\n",
    "            if self.transform:\n",
    "                dummy_img = self.transform(dummy_img)\n",
    "            return dummy_img, self.labels[idx], img_path\n",
    "\n",
    "def prepare_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    real_images = [os.path.join(Config.REAL_IMAGES, f) for f in os.listdir(Config.REAL_IMAGES) if f.endswith(('.jpg', '.png'))]\n",
    "    fake_images = [os.path.join(Config.FAKE_IMAGES, f) for f in os.listdir(Config.FAKE_IMAGES) if f.endswith(('.jpg', '.png'))]\n",
    "    \n",
    "    real_labels = [0] * len(real_images)\n",
    "    fake_labels = [1] * len(fake_images)\n",
    "    \n",
    "    all_images = real_images + fake_images\n",
    "    all_labels = real_labels + fake_labels\n",
    "    \n",
    "    indices = np.random.permutation(len(all_images))\n",
    "    all_images = [all_images[i] for i in indices]\n",
    "    all_labels = [all_labels[i] for i in indices]\n",
    "    \n",
    "    n_total = len(all_images)\n",
    "    n_train = int(0.7 * n_total)\n",
    "    n_val = int(0.15 * n_total)\n",
    "    \n",
    "    train_images = all_images[:n_train]\n",
    "    train_labels = all_labels[:n_train]\n",
    "    \n",
    "    val_images = all_images[n_train:n_train+n_val]\n",
    "    val_labels = all_labels[n_train:n_train+n_val]\n",
    "    \n",
    "    test_images = all_images[n_train+n_val:]\n",
    "    test_labels = all_labels[n_train+n_val:]\n",
    "    \n",
    "    train_dataset = ImageDataset(train_images, train_labels, transform)\n",
    "    val_dataset = ImageDataset(val_images, val_labels, transform)\n",
    "    test_dataset = ImageDataset(test_images, test_labels, transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, \n",
    "                             num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                           num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=Config.BATCH_SIZE, \n",
    "                            num_workers=2, pin_memory=True)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader, test_dataset\n",
    "\n",
    "# 2. Model Definition\n",
    "class FakeImageDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeImageDetector, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        \n",
    "        for module in self.resnet.modules():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                module.inplace = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x.clone())\n",
    "    \n",
    "    def get_activation_map(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x.clone())\n",
    "        x = self.resnet.maxpool(x)\n",
    "        \n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 3. Training Functions\n",
    "def train_model(train_loader, val_loader):\n",
    "    model = FakeImageDetector().to(Config.DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    \n",
    "    for epoch in range(Config.NUM_EPOCHS):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels, _ in tqdm(train_loader):\n",
    "            images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            del images, labels, outputs, loss\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        train_acc = 100.0 * correct / total\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, _ in val_loader:\n",
    "                images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                \n",
    "                del images, labels, outputs, loss\n",
    "                torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        val_acc = 100.0 * correct / total\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{Config.NUM_EPOCHS}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), os.path.join(Config.OUTPUT_DIR, 'best_model.pth'))\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 4. SHAP Analysis\n",
    "def explain_model_with_shap(model, test_dataset):\n",
    "    model.eval()\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            module.inplace = False\n",
    "    \n",
    "    background = []\n",
    "    background_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "    \n",
    "    for i, (img, _, _) in enumerate(background_loader):\n",
    "        if i >= Config.NUM_BACKGROUND:\n",
    "            break\n",
    "        background.append(img.squeeze(0).cpu().numpy())\n",
    "    \n",
    "    background = np.array(background)\n",
    "    \n",
    "    class ModelWrapper(nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super(ModelWrapper, self).__init__()\n",
    "            self.model = model\n",
    "            \n",
    "        def forward(self, x):\n",
    "            x = x.clone()\n",
    "            return self.model(x)\n",
    "    \n",
    "    wrapped_model = ModelWrapper(model).to(Config.DEVICE)\n",
    "    \n",
    "    e = shap.GradientExplainer(wrapped_model, torch.tensor(background).to(Config.DEVICE))\n",
    "    \n",
    "    test_images = []\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    image_paths = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for i, (img, label, path) in enumerate(test_loader):\n",
    "        if i >= Config.SHAP_SAMPLES:\n",
    "            break\n",
    "        test_images.append(img.squeeze(0).cpu().numpy())\n",
    "        image_paths.append(path)\n",
    "        true_labels.append(label.item())\n",
    "    \n",
    "    test_images = np.array(test_images)\n",
    "    \n",
    "    batch_size = 5\n",
    "    num_batches = (len(test_images) + batch_size - 1) // batch_size\n",
    "    all_shap_values = [[] for _ in range(2)]\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(test_images))\n",
    "        \n",
    "        batch_images = test_images[start_idx:end_idx]\n",
    "        batch_tensor = torch.tensor(batch_images, requires_grad=True).to(Config.DEVICE)\n",
    "        \n",
    "        try:\n",
    "            batch_shap_values = e.shap_values(batch_tensor, nsamples=50)\n",
    "            for i in range(2):\n",
    "                all_shap_values[i].extend(batch_shap_values[i])  # Already NumPy arrays\n",
    "        except Exception as ex:\n",
    "            print(f\"Error in batch {batch_idx}: {ex}\")\n",
    "            for i in range(end_idx - start_idx):\n",
    "                for j in range(2):\n",
    "                    zeros = np.zeros_like(batch_images[0])\n",
    "                    all_shap_values[j].append(zeros)\n",
    "        \n",
    "        del batch_tensor\n",
    "        if 'batch_shap_values' in locals():\n",
    "            del batch_shap_values\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        gc.collect()\n",
    "    \n",
    "    all_shap_values = [np.array(vals) for vals in all_shap_values]\n",
    "    \n",
    "    return all_shap_values, test_images, image_paths, true_labels\n",
    "\n",
    "# 5. Description Generator\n",
    "def generate_descriptions(model, shap_values, test_images, image_paths, true_labels):\n",
    "    descriptions = []\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    batch_size = 8\n",
    "    num_batches = (len(test_images) + batch_size - 1) // batch_size\n",
    "    all_predictions = []\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min((batch_idx + 1) * batch_size, len(test_images))\n",
    "        \n",
    "        batch_images = test_images[start_idx:end_idx]\n",
    "        batch_tensor = torch.tensor(batch_images).to(Config.DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_tensor)\n",
    "            _, predicted = outputs.max(1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "        del batch_tensor, outputs\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    predicted = np.array(all_predictions)\n",
    "    \n",
    "    shap_values_class0 = shap_values[0]\n",
    "    shap_values_class1 = shap_values[1]\n",
    "    \n",
    "    for i in range(len(test_images)):\n",
    "        img_path = image_paths[i][0]\n",
    "        true_label = true_labels[i]\n",
    "        pred_label = predicted[i]\n",
    "        \n",
    "        result = {\n",
    "            \"image_path\": img_path,\n",
    "            \"true_label\": \"Real\" if true_label == 0 else \"Fake\",\n",
    "            \"predicted_label\": \"Real\" if pred_label == 0 else \"Fake\", \n",
    "            \"correct_prediction\": bool(true_label == pred_label),\n",
    "        }\n",
    "        \n",
    "        if pred_label == 1:\n",
    "            shap_vals = shap_values_class1[i]\n",
    "            \n",
    "            attribution = np.sum(np.abs(shap_vals), axis=0)\n",
    "            \n",
    "            height, width = attribution.shape\n",
    "            h_quadrants = [\n",
    "                (0, height//2),\n",
    "                (height//2, height)\n",
    "            ]\n",
    "            w_quadrants = [\n",
    "                (0, width//2),\n",
    "                (width//2, width)\n",
    "            ]\n",
    "            \n",
    "            region_scores = {}\n",
    "            for h_idx, (h_start, h_end) in enumerate(h_quadrants):\n",
    "                for w_idx, (w_start, w_end) in enumerate(w_quadrants):\n",
    "                    region_name = f\"quadrant_{h_idx}_{w_idx}\"\n",
    "                    region_scores[region_name] = np.sum(attribution[h_start:h_end, w_start:w_end])\n",
    "            \n",
    "            sorted_regions = sorted(region_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "            top_regions = [r[0] for r in sorted_regions[:2]]\n",
    "            \n",
    "            artifacts = []\n",
    "            \n",
    "            if \"quadrant_0_0\" in top_regions:\n",
    "                artifacts.append(\"unusual texture patterns in the top-left region\")\n",
    "            if \"quadrant_0_1\" in top_regions:\n",
    "                artifacts.append(\"inconsistent lighting in the top-right region\")\n",
    "            if \"quadrant_1_0\" in top_regions:\n",
    "                artifacts.append(\"edge artifacts in the bottom-left region\")\n",
    "            if \"quadrant_1_1\" in top_regions:\n",
    "                artifacts.append(\"unnatural color distribution in the bottom-right region\")\n",
    "            \n",
    "            if not artifacts:\n",
    "                artifacts = [\"subtle visual inconsistencies throughout the image\"]\n",
    "            \n",
    "            description = f\"This image appears to be AI-generated due to {', '.join(artifacts[:-1])}\"\n",
    "            if len(artifacts) > 1:\n",
    "                description += f\", and {artifacts[-1]}\"\n",
    "            else:\n",
    "                description = f\"This image appears to be AI-generated due to {artifacts[0]}\"\n",
    "            \n",
    "            result[\"description\"] = description\n",
    "            result[\"key_artifacts\"] = artifacts\n",
    "            \n",
    "        else:\n",
    "            description = \"This appears to be an authentic photograph with natural lighting, consistent textures, and realistic details.\"\n",
    "            result[\"description\"] = description\n",
    "        \n",
    "        descriptions.append(result)\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    return descriptions\n",
    "\n",
    "# 6. Visualization Functions\n",
    "def visualize_shap_results(image_idx, shap_values, test_images, descriptions):\n",
    "    test_image = test_images[image_idx]\n",
    "    \n",
    "    description = descriptions[image_idx]\n",
    "    \n",
    "    if description[\"predicted_label\"] == \"Fake\":\n",
    "        shap_vals = shap_values[1][image_idx]\n",
    "    else:\n",
    "        shap_vals = shap_values[0][image_idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 7))\n",
    "    \n",
    "    img_display = np.transpose(test_image, (1, 2, 0))\n",
    "    img_display = (img_display - img_display.min()) / (img_display.max() - img_display.min() + 1e-8)\n",
    "    axes[0].imshow(img_display)\n",
    "    axes[0].set_title(f\"Original Image\\nTrue: {description['true_label']}, Pred: {description['predicted_label']}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    shap_visualization = np.sum(np.abs(shap_vals), axis=0)\n",
    "    shap_visualization = shap_visualization / (shap_visualization.max() + 1e-8)\n",
    "    \n",
    "    axes[1].imshow(shap_visualization, cmap='hot')\n",
    "    axes[1].set_title(\"SHAP Attribution Map\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.figtext(0.5, 0.01, description[\"description\"], ha=\"center\", fontsize=12, \n",
    "                bbox={\"facecolor\":\"white\", \"alpha\":0.5, \"pad\":5})\n",
    "    \n",
    "    plt.savefig(os.path.join(Config.OUTPUT_DIR, f\"shap_vis_{image_idx}.png\"), dpi=100)\n",
    "    plt.close()\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "# 7. Main Function\n",
    "def main():\n",
    "    os.makedirs(Config.OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    train_loader, val_loader, test_loader, test_dataset = prepare_data()\n",
    "    \n",
    "    model = FakeImageDetector().to(Config.DEVICE)\n",
    "    \n",
    "    if os.path.exists(os.path.join(Config.OUTPUT_DIR, 'best_model.pth')):\n",
    "        state_dict = torch.load(os.path.join(Config.OUTPUT_DIR, 'best_model.pth'), \n",
    "                               map_location=Config.DEVICE, weights_only=True)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pre-trained model.\")\n",
    "    else:\n",
    "        model = train_model(train_loader, val_loader)\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            module.inplace = False\n",
    "    \n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in test_loader:\n",
    "            images, labels = images.to(Config.DEVICE), labels.to(Config.DEVICE)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            \n",
    "            del images, labels, outputs\n",
    "            torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "    \n",
    "    test_acc = 100.0 * correct / total\n",
    "    print(f'Test Accuracy: {test_acc:.2f}%')\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    print(\"Generating SHAP explanations...\")\n",
    "    shap_values, test_images, image_paths, true_labels = explain_model_with_shap(model, test_dataset)\n",
    "    \n",
    "    print(\"Generating descriptions based on SHAP values...\")\n",
    "    descriptions = generate_descriptions(model, shap_values, test_images, image_paths, true_labels)\n",
    "    \n",
    "    with open(os.path.join(Config.OUTPUT_DIR, 'descriptions.json'), 'w') as f:\n",
    "        json.dump(descriptions, f, indent=4)\n",
    "    \n",
    "    num_vis = min(5, len(test_images))\n",
    "    for i in range(num_vis):\n",
    "        print(f\"Visualizing results for image {i+1}/{num_vis}...\")\n",
    "        visualize_shap_results(i, shap_values, test_images, descriptions)\n",
    "    \n",
    "    print(\"Done! Results saved to:\", Config.OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T03:33:41.214345Z",
     "iopub.status.busy": "2025-04-15T03:33:41.213824Z",
     "iopub.status.idle": "2025-04-15T03:34:26.798767Z",
     "shell.execute_reply": "2025-04-15T03:34:26.797525Z",
     "shell.execute_reply.started": "2025-04-15T03:33:41.214320Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-15 03:33:44.781008: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744688024.950881      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744688025.005340      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e605badc84c42db9d79b930655e4f8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ee2ac5abd3447382467f02bf8bf5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11eb8476ecd345fda29f510f17bf3060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51be5f1f81aa4bc8b3644f269d8b5b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a223e52a1f542b799dfd46423a2ff81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88163b130d6349168c967d091029bc5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc5f9ea7c9449eeaaded58324a1b16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb1de807b1c44399be5a57beb9f71b69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddad5c408a3044cdb79b6d1d051537a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models loaded successfully\n",
      "Found 2041 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1/409:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VLM model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c70081d0a7a4316ad5ecfe91f8c7fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e34c10e7b641b38826b9bc8595fe9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc7e986374645b6a2b1d5535704b6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009912074c3c486c855e61fc3c978872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a47a481469841b1adf3626fe1167769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f2ea0d8f524bf0b67496de61b19d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a017be020fbf4b859283ca80de9be399",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32804cfcfa814682830e8edb5ecec620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLM model loaded\n",
      "VLM model unloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1/409:  20%|██        | 1/5 [00:08<00:34,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VLM model...\n",
      "VLM model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1/409:  40%|████      | 2/5 [00:11<00:16,  5.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLM model unloaded\n",
      "Loading VLM model...\n",
      "VLM model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1/409:  60%|██████    | 3/5 [00:14<00:08,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLM model unloaded\n",
      "Loading VLM model...\n",
      "VLM model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1/409:  80%|████████  | 4/5 [00:17<00:03,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VLM model unloaded\n",
      "Loading VLM model...\n",
      "VLM model loaded\n",
      "VLM model unloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 1/409: 100%|██████████| 5/5 [00:20<00:00,  4.05s/it]\n",
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved to outputs/fake_detection_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 2/409:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VLM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batch 2/409:   0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31/4061590589.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 652\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_31/4061590589.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# Process images with error handling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m             \u001b[0;31m# Save results as Excel file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/4061590589.py\u001b[0m in \u001b[0;36mbatch_process\u001b[0;34m(self, image_paths, output_dir, batch_size)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Processing batch {i//batch_size + 1}/{total_batches}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m                     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                     \u001b[0mbatch_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/4061590589.py\u001b[0m in \u001b[0;36mprocess_image\u001b[0;34m(self, image_path)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# Using a simpler approach for feature importance instead of SHAP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mheatmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_array\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mdescription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_vlm_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheatmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfidence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;31m# Determine if this is from the real or fake directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/4061590589.py\u001b[0m in \u001b[0;36mgenerate_vlm_description\u001b[0;34m(self, img, heatmap, is_fake, confidence)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;34m\"\"\"Generate a description using a VLM, incorporating importance map insights.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_vlm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31/4061590589.py\u001b[0m in \u001b[0;36m_load_vlm_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading VLM model...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Salesforce/blip-image-captioning-base\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvlm_processor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             self.vlm_model = BlipForConditionalGeneration.from_pretrained(\n\u001b[1;32m    131\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/processing_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m             )\n\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mprocessor_class\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m             return processor_class.from_pretrained(\n\u001b[0m\u001b[1;32m    348\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arguments_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1080\u001b[0;31m         \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_processor_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m         \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_args_and_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessor_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/processing_utils.py\u001b[0m in \u001b[0;36mget_processor_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    746\u001b[0m                 \u001b[0;31m# because making it part of processor-config break BC.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0;31m# Processors in older version do not accept any kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m                 resolved_chat_template_file = cached_file(\n\u001b[0m\u001b[1;32m    749\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m                     \u001b[0mchat_template_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \"\"\"\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcached_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_files\u001b[0;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_filenames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# This is slightly better for only 1 file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m             hf_hub_download(\n\u001b[0m\u001b[1;32m    425\u001b[0m                 \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m    959\u001b[0m         )\n\u001b[1;32m    960\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m         return _hf_hub_download_to_cache_dir(\n\u001b[0m\u001b[1;32m    962\u001b[0m             \u001b[0;31m# Destination\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[0;31m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;31m# If we can't, a HEAD request error is returned.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m     (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) = _get_metadata_or_catch_error(\n\u001b[0m\u001b[1;32m   1025\u001b[0m         \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1483\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1484\u001b[0;31m                 metadata = get_hf_file_metadata(\n\u001b[0m\u001b[1;32m   1485\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m     \u001b[0;31m# Retrieve metadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m   1402\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;31m# Recursively follow relative redirects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfollow_relative_redirects\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         response = _request_wrapper(\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Send: {_curlify(request)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequestException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_AMZN_TRACE_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;31m# Receive the response from the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# Get the response from http.client.HTTPConnection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1393\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1395\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1396\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1312\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1314\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1164\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1166\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import shap\n",
    "import gc\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from tqdm import tqdm\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration, CLIPModel, CLIPProcessor\n",
    "\n",
    "# Configuration\n",
    "class Config:\n",
    "    DATA_ROOT = \"/kaggle/input/real-vs-fake-faces/\"\n",
    "    REAL_IMAGES = os.path.join(DATA_ROOT, \"real/\")\n",
    "    FAKE_IMAGES = os.path.join(DATA_ROOT, \"fake/\")\n",
    "    OUTPUT_DIR = \"outputs/\"\n",
    "    IMAGE_SIZE = 224\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 5\n",
    "    LEARNING_RATE = 3e-4\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    NUM_BACKGROUND = 50\n",
    "    SHAP_SAMPLES = 25\n",
    "\n",
    "# FakeImageDetector\n",
    "class FakeImageDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeImageDetector, self).__init__()\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        num_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(num_features, 256),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "        \n",
    "        # Ensure all ReLU operations are not in-place to work with SHAP\n",
    "        for module in self.resnet.modules():\n",
    "            if isinstance(module, nn.ReLU):\n",
    "                module.inplace = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x.clone())\n",
    "    \n",
    "    def get_activation_map(self, x):\n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x.clone())\n",
    "        x = self.resnet.maxpool(x)\n",
    "        \n",
    "        x = self.resnet.layer1(x)\n",
    "        x = self.resnet.layer2(x)\n",
    "        x = self.resnet.layer3(x)\n",
    "        x = self.resnet.layer4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# VLM Description Generator\n",
    "class VLMDescriptionGenerator:\n",
    "    def __init__(self, fake_detector_path=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        if not os.path.exists(Config.OUTPUT_DIR):\n",
    "            os.makedirs(Config.OUTPUT_DIR)\n",
    "        \n",
    "        # Create image and heatmap directories\n",
    "        self.images_dir = os.path.join(Config.OUTPUT_DIR, \"processed_images\")\n",
    "        self.heatmaps_dir = os.path.join(Config.OUTPUT_DIR, \"heatmaps\")\n",
    "        self.overlays_dir = os.path.join(Config.OUTPUT_DIR, \"overlays\")\n",
    "        \n",
    "        os.makedirs(self.images_dir, exist_ok=True)\n",
    "        os.makedirs(self.heatmaps_dir, exist_ok=True)\n",
    "        os.makedirs(self.overlays_dir, exist_ok=True)\n",
    "        \n",
    "        # Load the fake detector model\n",
    "        if fake_detector_path is None:\n",
    "            fake_detector_path = os.path.join(Config.OUTPUT_DIR, 'best_model.pth')\n",
    "        \n",
    "        # Check if the model file exists before loading\n",
    "        if not os.path.exists(fake_detector_path):\n",
    "            raise FileNotFoundError(f\"Fake detector model not found at {fake_detector_path}\")\n",
    "        \n",
    "        self.fake_detector = FakeImageDetector().to(self.device)\n",
    "        self.fake_detector.load_state_dict(torch.load(fake_detector_path, map_location=self.device))\n",
    "        self.fake_detector.eval()\n",
    "        \n",
    "        # Load CLIP for feature extraction\n",
    "        print(\"Loading CLIP model...\")\n",
    "        self.clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "        self.clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.clip_model.eval()\n",
    "        \n",
    "        # Initialize DataFrame for results\n",
    "        self.results_df = pd.DataFrame(columns=[\n",
    "            'image_filename',\n",
    "            'image_path',\n",
    "            'processed_image_path',\n",
    "            'heatmap_path',\n",
    "            'overlay_path',\n",
    "            'is_fake',\n",
    "            'confidence',\n",
    "            'description',\n",
    "            'manual_description',  # This will be empty initially\n",
    "            'source_label'         # Whether it came from real or fake folder\n",
    "        ])\n",
    "        \n",
    "        # Placeholders for VLM models\n",
    "        self.vlm_processor = None\n",
    "        self.vlm_model = None\n",
    "        \n",
    "        print(\"Models loaded successfully\")\n",
    "        \n",
    "    def _load_vlm_model(self):\n",
    "        \"\"\"Load VLM model on demand to save memory\"\"\"\n",
    "        if self.vlm_model is None:\n",
    "            print(\"Loading VLM model...\")\n",
    "            model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "            self.vlm_processor = AutoProcessor.from_pretrained(model_name)\n",
    "            self.vlm_model = BlipForConditionalGeneration.from_pretrained(\n",
    "                model_name,\n",
    "                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,\n",
    "                low_cpu_mem_usage=True,\n",
    "                device_map=\"auto\" if self.device.type == 'cuda' else None\n",
    "            )\n",
    "            if self.device.type != 'cuda':\n",
    "                self.vlm_model = self.vlm_model.to(self.device)\n",
    "            self.vlm_model.eval()\n",
    "            print(\"VLM model loaded\")\n",
    "        \n",
    "    def _unload_vlm_model(self):\n",
    "        \"\"\"Unload VLM model to free memory\"\"\"\n",
    "        if self.vlm_model is not None:\n",
    "            del self.vlm_processor\n",
    "            del self.vlm_model\n",
    "            self.vlm_processor = None\n",
    "            self.vlm_model = None\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            print(\"VLM model unloaded\")\n",
    "    \n",
    "    def process_image(self, image_path):\n",
    "        \"\"\"Process a single image and generate descriptions.\"\"\"\n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            img_tensor = self.preprocess_for_fake_detector(img)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.fake_detector(img_tensor.unsqueeze(0).to(self.device))\n",
    "                _, predicted = outputs.max(1)\n",
    "                is_fake = bool(predicted.item())\n",
    "                confidence = torch.softmax(outputs, dim=1)[0][predicted.item()].item()\n",
    "            \n",
    "            del outputs\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Using a simpler approach for feature importance instead of SHAP\n",
    "            heatmap = self.get_feature_importance(img_tensor, img_array.shape[:2])\n",
    "            description = self.generate_vlm_description(img, heatmap, is_fake, confidence)\n",
    "            \n",
    "            # Determine if this is from the real or fake directory\n",
    "            if \"real\" in image_path.lower():\n",
    "                source_label = \"real\"\n",
    "            elif \"fake\" in image_path.lower():\n",
    "                source_label = \"fake\"\n",
    "            else:\n",
    "                source_label = \"unknown\"\n",
    "            \n",
    "            # Save processed images\n",
    "            image_filename = os.path.basename(image_path)\n",
    "            base_name = os.path.splitext(image_filename)[0]\n",
    "            \n",
    "            # Save processed image\n",
    "            processed_img_path = os.path.join(self.images_dir, f\"{base_name}_processed.jpg\")\n",
    "            img.save(processed_img_path)\n",
    "            \n",
    "            # Save heatmap\n",
    "            heatmap_path = os.path.join(self.heatmaps_dir, f\"{base_name}_heatmap.jpg\")\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.imshow(heatmap, cmap='jet')\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(heatmap_path)\n",
    "            plt.close()\n",
    "            \n",
    "            # Create and save overlay\n",
    "            heatmap_uint8 = (heatmap * 255).astype(np.uint8)\n",
    "            heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
    "            heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "            heatmap_resized = cv2.resize(heatmap_colored, (img_array.shape[1], img_array.shape[0]))\n",
    "            overlay = cv2.addWeighted(img_array, 0.7, heatmap_resized, 0.3, 0)\n",
    "            \n",
    "            overlay_path = os.path.join(self.overlays_dir, f\"{base_name}_overlay.jpg\")\n",
    "            cv2.imwrite(overlay_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            return {\n",
    "                \"image_filename\": image_filename,\n",
    "                \"image_path\": image_path,\n",
    "                \"processed_image_path\": processed_img_path,\n",
    "                \"heatmap_path\": heatmap_path,\n",
    "                \"overlay_path\": overlay_path,\n",
    "                \"is_fake\": is_fake,\n",
    "                \"confidence\": confidence,\n",
    "                \"description\": description,\n",
    "                \"manual_description\": \"\",  # Empty initially\n",
    "                \"source_label\": source_label,\n",
    "                \"heatmap\": heatmap  # Keep heatmap for visualization if needed\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "            return {\n",
    "                \"image_filename\": os.path.basename(image_path),\n",
    "                \"image_path\": image_path,\n",
    "                \"processed_image_path\": \"\",\n",
    "                \"heatmap_path\": \"\",\n",
    "                \"overlay_path\": \"\",\n",
    "                \"is_fake\": None,\n",
    "                \"confidence\": 0.0,\n",
    "                \"description\": f\"Error processing image: {str(e)}\",\n",
    "                \"manual_description\": \"\",\n",
    "                \"source_label\": \"error\",\n",
    "                \"heatmap\": np.zeros((224, 224))\n",
    "            }\n",
    "    \n",
    "    def preprocess_for_fake_detector(self, img):\n",
    "        \"\"\"Preprocess image for the fake detector model.\"\"\"\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        return transform(img)\n",
    "    \n",
    "    def get_feature_importance(self, img_tensor, original_shape):\n",
    "        \"\"\"Get feature importance using a gradient-based approach instead of SHAP.\"\"\"\n",
    "        try:\n",
    "            img_tensor = img_tensor.to(self.device).unsqueeze(0)\n",
    "            img_tensor.requires_grad_(True)\n",
    "            \n",
    "            # Forward pass with gradient\n",
    "            self.fake_detector.eval()\n",
    "            outputs = self.fake_detector(img_tensor)\n",
    "            \n",
    "            # Get the predicted class\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            # Zero gradients\n",
    "            self.fake_detector.zero_grad()\n",
    "            \n",
    "            # Backward pass for the predicted class\n",
    "            one_hot = torch.zeros_like(outputs)\n",
    "            one_hot[0, predicted] = 1\n",
    "            outputs.backward(gradient=one_hot)\n",
    "            \n",
    "            # Get gradients\n",
    "            gradients = img_tensor.grad.cpu().detach().numpy()[0]\n",
    "            \n",
    "            # Take absolute value and average over channels\n",
    "            importance = np.mean(np.abs(gradients), axis=0)\n",
    "            \n",
    "            # Normalize importance scores\n",
    "            importance_min, importance_max = importance.min(), importance.max()\n",
    "            if importance_max > importance_min:\n",
    "                importance = (importance - importance_min) / (importance_max - importance_min)\n",
    "            \n",
    "            # Resize to original shape\n",
    "            importance_resized = cv2.resize(importance, (original_shape[1], original_shape[0]))\n",
    "            \n",
    "            return importance_resized\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating feature importance: {e}\")\n",
    "            return np.zeros(original_shape)\n",
    "    \n",
    "    def generate_vlm_description(self, img, heatmap, is_fake, confidence):\n",
    "        \"\"\"Generate a description using a VLM, incorporating importance map insights.\"\"\"\n",
    "        try:\n",
    "            self._load_vlm_model()\n",
    "            \n",
    "            img_array = np.array(img)\n",
    "            # Create colored heatmap overlay\n",
    "            heatmap_uint8 = (heatmap * 255).astype(np.uint8)\n",
    "            heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
    "            heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Resize heatmap to match image dimensions\n",
    "            heatmap_colored = cv2.resize(heatmap_colored, (img_array.shape[1], img_array.shape[0]))\n",
    "            \n",
    "            # Create overlay image\n",
    "            overlay = cv2.addWeighted(img_array, 0.7, heatmap_colored, 0.3, 0)\n",
    "            overlay_img = Image.fromarray(overlay.astype('uint8'))\n",
    "            \n",
    "            # Prepare inputs for VLM\n",
    "            inputs = self.vlm_processor(\n",
    "                images=overlay_img,\n",
    "                text=f\"This image appears to be {'AI-generated' if is_fake else 'authentic'} with {confidence:.1%} confidence. Describe what makes it look {'artificial' if is_fake else 'real'}:\",\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            # Generate description\n",
    "            with torch.no_grad():\n",
    "                outputs = self.vlm_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                )\n",
    "            \n",
    "            # Process and format description\n",
    "            description = self.vlm_processor.decode(outputs[0], skip_special_tokens=True)\n",
    "            prompt = f\"This image appears to be {'AI-generated' if is_fake else 'authentic'} with {confidence:.1%} confidence. Describe what makes it look {'artificial' if is_fake else 'real'}:\"\n",
    "            if prompt in description:\n",
    "                description = description.replace(prompt, \"\").strip()\n",
    "            \n",
    "            # Clean up to save memory\n",
    "            del inputs, outputs, overlay, overlay_img\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            self._unload_vlm_model()\n",
    "            \n",
    "            return description\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating VLM description: {e}\")\n",
    "            self._unload_vlm_model()\n",
    "            return f\"Unable to generate description due to error: {str(e)}\"\n",
    "    \n",
    "    def batch_process(self, image_paths, output_dir=None, batch_size=10):\n",
    "        \"\"\"Process a batch of images and save the results.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        if output_dir and not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # Calculate total number of batches for progress tracking\n",
    "        total_batches = (len(image_paths) - 1) // batch_size + 1\n",
    "        \n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch = image_paths[i:i+batch_size]\n",
    "            batch_results = []\n",
    "            \n",
    "            for img_path in tqdm(batch, desc=f\"Processing batch {i//batch_size + 1}/{total_batches}\"):\n",
    "                try:\n",
    "                    result = self.process_image(img_path)\n",
    "                    results.append(result)\n",
    "                    batch_results.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "                    # Continue with next image on error\n",
    "                    continue\n",
    "            \n",
    "            # Add batch results to DataFrame\n",
    "            batch_df = pd.DataFrame(batch_results)\n",
    "            self.results_df = pd.concat([self.results_df, batch_df], ignore_index=True)\n",
    "            \n",
    "            # Save updated CSV after each batch\n",
    "            if output_dir:\n",
    "                csv_path = os.path.join(output_dir, \"fake_detection_results.csv\")\n",
    "                # Create a copy of the dataframe without the heatmap column for CSV saving\n",
    "                save_df = self.results_df.drop(columns=['heatmap'], errors='ignore')\n",
    "                save_df.to_csv(csv_path, index=False)\n",
    "                print(f\"Updated CSV saved to {csv_path}\")\n",
    "            \n",
    "            # Clear memory between batches\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Save final results\n",
    "        if output_dir:\n",
    "            # Create HTML file that shows the images, heatmaps, and descriptions\n",
    "            self.create_html_report(output_dir)\n",
    "        \n",
    "        return results\n",
    "\n",
    "    def create_html_report(self, output_dir):\n",
    "        \"\"\"Create an HTML report with all the images, heatmaps, and descriptions.\"\"\"\n",
    "        html_path = os.path.join(output_dir, \"detection_results.html\")\n",
    "        \n",
    "        html_content = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head>\n",
    "            <title>Fake Image Detection Results</title>\n",
    "            <style>\n",
    "                body { font-family: Arial, sans-serif; margin: 20px; }\n",
    "                .entry { border: 1px solid #ddd; margin: 20px 0; padding: 15px; border-radius: 5px; }\n",
    "                .images { display: flex; flex-wrap: wrap; gap: 10px; }\n",
    "                .image-container { margin: 10px; }\n",
    "                img { max-width: 300px; max-height: 300px; }\n",
    "                .metadata { margin-top: 15px; }\n",
    "                .fake { background-color: #ffdddd; }\n",
    "                .real { background-color: #ddffdd; }\n",
    "                .error { background-color: #dddddd; }\n",
    "                h2 { margin-top: 0; }\n",
    "                table { border-collapse: collapse; width: 100%; }\n",
    "                table, th, td { border: 1px solid #ddd; }\n",
    "                th, td { padding: 8px; text-align: left; }\n",
    "                tr:nth-child(even) { background-color: #f2f2f2; }\n",
    "            </style>\n",
    "        </head>\n",
    "        <body>\n",
    "            <h1>Fake Image Detection Results</h1>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add summary statistics\n",
    "        real_count = len(self.results_df[self.results_df['is_fake'] == False])\n",
    "        fake_count = len(self.results_df[self.results_df['is_fake'] == True])\n",
    "        error_count = len(self.results_df[self.results_df['is_fake'].isna()])\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "        <div class=\"summary\">\n",
    "            <h2>Summary</h2>\n",
    "            <p>Total images: {len(self.results_df)}</p>\n",
    "            <p>Detected as real: {real_count}</p>\n",
    "            <p>Detected as fake: {fake_count}</p>\n",
    "            <p>Processing errors: {error_count}</p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add individual entries\n",
    "        for _, row in self.results_df.iterrows():\n",
    "            # Determine the CSS class based on the fake detection result\n",
    "            if row['is_fake'] is None:\n",
    "                entry_class = \"error\"\n",
    "                status = \"Error\"\n",
    "            elif row['is_fake']:\n",
    "                entry_class = \"fake\"\n",
    "                status = \"Fake\"\n",
    "            else:\n",
    "                entry_class = \"real\"\n",
    "                status = \"Real\"\n",
    "            \n",
    "            confidence = row['confidence'] * 100 if row['confidence'] is not None else 0\n",
    "            \n",
    "            html_content += f\"\"\"\n",
    "            <div class=\"entry {entry_class}\">\n",
    "                <h2>{row['image_filename']} - {status} ({confidence:.1f}%)</h2>\n",
    "                <div class=\"images\">\n",
    "                    <div class=\"image-container\">\n",
    "                        <h3>Original</h3>\n",
    "                        <img src=\"{row['processed_image_path']}\" alt=\"Original Image\">\n",
    "                    </div>\n",
    "                    <div class=\"image-container\">\n",
    "                        <h3>Heatmap</h3>\n",
    "                        <img src=\"{row['heatmap_path']}\" alt=\"Heatmap\">\n",
    "                    </div>\n",
    "                    <div class=\"image-container\">\n",
    "                        <h3>Overlay</h3>\n",
    "                        <img src=\"{row['overlay_path']}\" alt=\"Overlay\">\n",
    "                    </div>\n",
    "                </div>\n",
    "                <div class=\"metadata\">\n",
    "                    <h3>Detection Details</h3>\n",
    "                    <p><strong>Source Label:</strong> {row['source_label']}</p>\n",
    "                    <p><strong>Description:</strong> {row['description']}</p>\n",
    "                    <p><strong>Manual Description:</strong> {row['manual_description'] if row['manual_description'] else \"N/A\"}</p>\n",
    "                </div>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "        \n",
    "        with open(html_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        print(f\"HTML report saved to {html_path}\")\n",
    "\n",
    "    def export_to_excel(self, output_path):\n",
    "        \"\"\"Export results to an Excel file with embedded images.\"\"\"\n",
    "        from openpyxl import Workbook\n",
    "        from openpyxl.drawing.image import Image as XLImage\n",
    "        from openpyxl.utils import get_column_letter\n",
    "        from PIL import Image as PILImage\n",
    "        import io\n",
    "        \n",
    "        print(\"Exporting to Excel...\")\n",
    "        wb = Workbook()\n",
    "        ws = wb.active\n",
    "        ws.title = \"Detection Results\"\n",
    "        \n",
    "        # Write headers\n",
    "        headers = [\n",
    "            'Image ID', 'Is Fake', 'Confidence', 'Source Label', \n",
    "            'Description', 'Manual Description'\n",
    "        ]\n",
    "        for col_num, header in enumerate(headers, 1):\n",
    "            ws.cell(row=1, column=col_num).value = header\n",
    "        \n",
    "        # Format header row\n",
    "        for col_num in range(1, len(headers) + 1):\n",
    "            cell = ws.cell(row=1, column=col_num)\n",
    "            cell.font = cell.font.copy(bold=True)\n",
    "        \n",
    "        # Add data and images\n",
    "        for idx, row in enumerate(self.results_df.iterrows(), 2):\n",
    "            row_data = row[1]\n",
    "            \n",
    "            # Add text data\n",
    "            ws.cell(row=idx, column=1).value = row_data['image_filename']\n",
    "            ws.cell(row=idx, column=2).value = str(row_data['is_fake'])\n",
    "            ws.cell(row=idx, column=3).value = f\"{row_data['confidence']:.2%}\" if row_data['confidence'] is not None else \"N/A\"\n",
    "            ws.cell(row=idx, column=4).value = row_data['source_label']\n",
    "            ws.cell(row=idx, column=5).value = row_data['description']\n",
    "            ws.cell(row=idx, column=6).value = row_data['manual_description']\n",
    "            \n",
    "        # Adjust column widths\n",
    "        for col_num in range(1, len(headers) + 1):\n",
    "            column = get_column_letter(col_num)\n",
    "            ws.column_dimensions[column].width = 30 if col_num >= 5 else 15\n",
    "        \n",
    "        # Save workbook\n",
    "        wb.save(output_path)\n",
    "        print(f\"Excel report saved to {output_path}\")\n",
    "    \n",
    "    def add_manual_description(self, image_filename, manual_description):\n",
    "        \"\"\"Add a manual description for an image.\"\"\"\n",
    "        idx = self.results_df[self.results_df['image_filename'] == image_filename].index\n",
    "        if len(idx) > 0:\n",
    "            self.results_df.loc[idx[0], 'manual_description'] = manual_description\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def visualize_result(self, result, save_path=None):\n",
    "        \"\"\"Visualize the result with the original image and heatmap overlay.\"\"\"\n",
    "        try:\n",
    "            # Skip visualization if no valid result\n",
    "            if result[\"is_fake\"] is None:\n",
    "                return None\n",
    "                \n",
    "            img = Image.open(result[\"image_path\"]).convert('RGB')\n",
    "            img_array = np.array(img)\n",
    "            \n",
    "            fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "            \n",
    "            # Original image\n",
    "            axs[0].imshow(img_array)\n",
    "            axs[0].set_title(\"Original Image\")\n",
    "            axs[0].axis('off')\n",
    "            \n",
    "            # Heatmap\n",
    "            heatmap_img = axs[1].imshow(result[\"heatmap\"], cmap='jet')\n",
    "            axs[1].set_title(\"Detection Heatmap\")\n",
    "            axs[1].axis('off')\n",
    "            fig.colorbar(heatmap_img, ax=axs[1], fraction=0.046, pad=0.04)\n",
    "            \n",
    "            # Overlay\n",
    "            heatmap_uint8 = (result[\"heatmap\"] * 255).astype(np.uint8)\n",
    "            heatmap_colored = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)\n",
    "            heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n",
    "            heatmap_resized = cv2.resize(heatmap_colored, (img_array.shape[1], img_array.shape[0]))\n",
    "            overlay = cv2.addWeighted(img_array, 0.7, heatmap_resized, 0.3, 0)\n",
    "            \n",
    "            axs[2].imshow(overlay)\n",
    "            axs[2].set_title(f\"Overlay - {'Fake' if result['is_fake'] else 'Real'} ({result['confidence']:.2%})\")\n",
    "            axs[2].axis('off')\n",
    "            \n",
    "            # Add description text - wrap text to avoid overflow\n",
    "            plt.figtext(0.5, 0.01, f\"Description: {result['description']}\", \n",
    "                       wrap=True, horizontalalignment='center', fontsize=10)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            if save_path:\n",
    "                plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "                \n",
    "            return fig\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error visualizing result: {e}\")\n",
    "            return None\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function adapted for notebook environment.\"\"\"\n",
    "    # Define input and output paths\n",
    "    input_path = Config.DATA_ROOT  # Base dataset directory\n",
    "    output_dir = Config.OUTPUT_DIR\n",
    "    batch_size = 5  # Smaller batch size to avoid memory issues\n",
    "    detector_path = os.path.join(Config.OUTPUT_DIR, 'best_model.pth')\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    # Check if model exists before proceeding\n",
    "    if not os.path.exists(detector_path):\n",
    "        print(f\"Model not found at {detector_path}. Please train the model first.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # Initialize the generator\n",
    "        generator = VLMDescriptionGenerator(fake_detector_path=detector_path)\n",
    "        \n",
    "        # Get list of images recursively\n",
    "        image_exts = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "        image_paths = []\n",
    "        if os.path.isdir(input_path):\n",
    "            for root, _, files in os.walk(input_path):\n",
    "                for f in files:\n",
    "                    if os.path.splitext(f.lower())[1] in image_exts:\n",
    "                        image_paths.append(os.path.join(root, f))\n",
    "        else:\n",
    "            if os.path.splitext(input_path.lower())[1] in image_exts:\n",
    "                image_paths = [input_path]\n",
    "        \n",
    "        print(f\"Found {len(image_paths)} images\")\n",
    "        \n",
    "        if not image_paths:\n",
    "            print(\"No images found. Please check the input path.\")\n",
    "            return\n",
    "        \n",
    "        # Process images with error handling\n",
    "        try:\n",
    "            results = generator.batch_process(image_paths, output_dir, batch_size)\n",
    "            \n",
    "            # Save results as Excel file\n",
    "            excel_path = os.path.join(output_dir, \"fake_detection_results.xlsx\")\n",
    "            generator.export_to_excel(excel_path)\n",
    "            \n",
    "            print(f\"Processed {len(results)} images. Results saved to:\")\n",
    "            print(f\"- CSV: {os.path.join(output_dir, 'fake_detection_results.csv')}\")\n",
    "            print(f\"- Excel: {excel_path}\")\n",
    "            print(f\"- HTML Report: {os.path.join(output_dir, 'detection_results.html')}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch processing: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main function: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T03:39:33.950092Z",
     "iopub.status.busy": "2025-04-15T03:39:33.949067Z",
     "iopub.status.idle": "2025-04-15T03:39:36.631286Z",
     "shell.execute_reply": "2025-04-15T03:39:36.630496Z",
     "shell.execute_reply.started": "2025-04-15T03:39:33.950063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/ (stored 0%)\n",
      "  adding: kaggle/working/outputs/ (stored 0%)\n",
      "  adding: kaggle/working/outputs/fake_detection_results.csv (deflated 76%)\n",
      "  adding: kaggle/working/outputs/shap_vis_1.png (deflated 0%)\n",
      "  adding: kaggle/working/outputs/shap_vis_3.png (deflated 0%)\n",
      "  adding: kaggle/working/outputs/shap_vis_0.png (deflated 0%)\n",
      "  adding: kaggle/working/outputs/overlays/ (stored 0%)\n",
      "  adding: kaggle/working/outputs/overlays/mid_233_1111_overlay.jpg (deflated 1%)\n",
      "  adding: kaggle/working/outputs/overlays/hard_32_1111_overlay.jpg (deflated 0%)\n",
      "  adding: kaggle/working/outputs/overlays/mid_345_1111_overlay.jpg (deflated 1%)\n",
      "  adding: kaggle/working/outputs/overlays/mid_161_0110_overlay.jpg (deflated 0%)\n",
      "  adding: kaggle/working/outputs/overlays/mid_200_1111_overlay.jpg (deflated 0%)\n",
      "  adding: kaggle/working/outputs/descriptions.json (deflated 94%)\n",
      "  adding: kaggle/working/outputs/processed_images/ (stored 0%)\n",
      "  adding: kaggle/working/outputs/processed_images/mid_233_1111_processed.jpg (deflated 0%)\n",
      "  adding: kaggle/working/outputs/processed_images/mid_345_1111_processed.jpg (deflated 1%)\n",
      "  adding: kaggle/working/outputs/processed_images/mid_200_1111_processed.jpg (deflated 1%)\n",
      "  adding: kaggle/working/outputs/processed_images/hard_32_1111_processed.jpg (deflated 1%)\n",
      "  adding: kaggle/working/outputs/processed_images/mid_161_0110_processed.jpg (deflated 0%)\n",
      "  adding: kaggle/working/outputs/shap_vis_2.png (deflated 0%)\n",
      "  adding: kaggle/working/outputs/shap_vis_4.png"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (deflated 0%)\n",
      "  adding: kaggle/working/outputs/best_model.pth (deflated 7%)\n",
      "  adding: kaggle/working/outputs/heatmaps/ (stored 0%)\n",
      "  adding: kaggle/working/outputs/heatmaps/mid_233_1111_heatmap.jpg (deflated 13%)\n",
      "  adding: kaggle/working/outputs/heatmaps/hard_32_1111_heatmap.jpg (deflated 13%)\n",
      "  adding: kaggle/working/outputs/heatmaps/mid_161_0110_heatmap.jpg (deflated 10%)\n",
      "  adding: kaggle/working/outputs/heatmaps/mid_200_1111_heatmap.jpg (deflated 12%)\n",
      "  adding: kaggle/working/outputs/heatmaps/mid_345_1111_heatmap.jpg (deflated 11%)\n",
      "  adding: kaggle/working/.virtual_documents/ (stored 0%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /kaggle/working/output.zip /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beb446e0",
   "metadata": {},
   "source": [
    "## 🔍 SHAP Analysis on Aggregated Video Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69cb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Load model and processor\n",
    "model.eval()\n",
    "model.cpu()  # SHAP works better on CPU\n",
    "\n",
    "# Pick a few samples from validation set\n",
    "sample_frames = []\n",
    "sample_labels = []\n",
    "for i in range(5):  # Pick 5 random samples\n",
    "    img_tensor, label = val_dataset[i]\n",
    "    sample_frames.append(img_tensor.numpy())\n",
    "    sample_labels.append(label)\n",
    "\n",
    "sample_frames = torch.tensor(sample_frames)\n",
    "\n",
    "# Define prediction function\n",
    "def predict(images):\n",
    "    with torch.no_grad():\n",
    "        images = torch.tensor(images).to(torch.float32)\n",
    "        outputs = model(pixel_values=images)\n",
    "        return outputs.logits.numpy()\n",
    "\n",
    "# Create SHAP explainer\n",
    "masker = shap.maskers.Image(\"inpaint_telea\", sample_frames[0].shape)\n",
    "explainer = shap.Explainer(predict, masker, output_names=[\"original\", \"manipulated\"])\n",
    "\n",
    "# Compute SHAP values\n",
    "shap_values = explainer(sample_frames)\n",
    "\n",
    "# Visualize\n",
    "shap.image_plot(shap_values, sample_frames)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2144922,
     "sourceId": 3575857,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "my_central_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
